READ ME - Please email Evan Russek, emr443@nyu.edu, any further questions.

The main text contains 3 types of tasks (latent learning, detour task, and policy revaluation).

Functions to simulate each model on each of these tasks is in the folder "simulation functions".  Functions containing the word "maze" simulate both the latent learning and policy revaluation task.  Functions ending in "detour" simulate the detour task.

Each of these functions takes in a number of runs (how many times to run the simulation for) as well as a set of model parameters.

The specific parameters differ for each model, but examples of how to set the parameters can be set are in the scripts in the folder "run simulations".

Plots of the median value function and policy can be made by uncommenting the final lines of these functions (which contain calls to functions such as "display policy").  The lines which save the game structure must also be uncommented (these must be commented however for code to run in parallel).

The models in the code correspond to models in the paper as follows:

vsr = SR-TD
vsr2 = SR-MB
vpunc = one-step lookahead
qsr = SR-Dyna
dynaQ = Dyna-Q
vsr_r = SR-TD with direct learning of the reward function for weights

Additionally, "qr_maze.m" contains code for the simulation in supplementary figure 1 and "rvw_maze.m" is simulation for supplementary figure 2.


